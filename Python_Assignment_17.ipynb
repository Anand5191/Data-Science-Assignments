{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksDv5AxlH3_f"
      },
      "outputs": [],
      "source": [
        "# Q1\n",
        "\n",
        "\"\"\"Web Scraping:\n",
        "Web scraping is an automated method used to extract large amounts of data from websites. This process involves retrieving the content of web pages,\n",
        "which is typically structured in HTML format, and converting it into a more usable format such as a spreadsheet or database. The primary components of\n",
        "web scraping include a crawler, which navigates the web to find specific data, and a scraper, which extracts the desired information from the HTML code of the web pages.\n",
        "\n",
        "Uses:\n",
        "Web scraping is employed for various reasons, primarily to gather data efficiently and effectively without manual effort. It allows users to collect vast amounts of\n",
        "information quickly, which can be particularly useful for tasks that require real-time data analysis or monitoring. By automating the data collection process,\n",
        "organizations can save time and resources while ensuring accuracy in their datasets.\n",
        "\n",
        "Areas Where Web Scraping is Used:\n",
        "Price Monitoring: Companies utilize web scraping to track product prices across different e-commerce platforms. This helps them adjust their pricing strategies based\n",
        "on competitor pricing and market trends.\n",
        "\n",
        "Market Research: Businesses leverage web scraping to gather insights about consumer behavior and preferences by analyzing large volumes of data from various sources\n",
        "such as reviews, social media posts, and forums.\n",
        "\n",
        "News Monitoring: Organizations scrape news websites to stay updated on current events relevant to their industry. This enables them to respond promptly to news that\n",
        "may impact their operations or reputation.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\n",
        "\"\"\"1. Manual Web Scraping\n",
        "Manual web scraping involves the process of manually copying and pasting data from a website into a document or spreadsheet. This method is straightforward but\n",
        "can be extremely time-consuming and prone to errors, especially when dealing with large amounts of data. It is often used for small-scale projects or when\n",
        "automated tools cannot bypass anti-scraping measures.\n",
        "\n",
        "2. Automated Web Scraping\n",
        "Automated web scraping utilizes software tools or scripts to collect data from websites without human intervention. This method is more efficient than manual scraping\n",
        "and can handle large volumes of data quickly. Here are some common techniques within automated web scraping:\n",
        "\n",
        "A. Web Scraping Libraries B. Headless Browsers C. APIs\n",
        "\n",
        "3. Optical Character Recognition (OCR)\n",
        "Optical Character Recognition (OCR) is used when data is embedded in images or scanned documents on web pages. OCR technology converts these non-text formats into\n",
        "machine-readable text, allowing users to extract information from images.\n",
        "\n",
        "4. HTML Parsing\n",
        "HTML parsing involves analyzing the structure of a webpage’s HTML code to identify specific elements for extraction. This method typically requires knowledge of HTML\n",
        "and may involve using parsers compatible with programming languages such as Python (e.g., lxml).\n",
        "\n",
        "5. DOM Parsing\n",
        "Document Object Model (DOM) parsing allows developers to interact with the structure of an HTML document programmatically after it has been loaded in a browser environment.\n",
        "This method enables precise extraction of elements based on their attributes or hierarchy within the DOM tree.\n",
        "\n",
        "6. Hybrid Approaches\n",
        "Hybrid approaches combine both automated and manual techniques to enhance data collection efficiency and accuracy. For instance, automated scrapers might gather most of the\n",
        "required data, while manual checks are performed afterward to verify accuracy or fill in missing information.\"\"\""
      ],
      "metadata": {
        "id": "FaZ4YJkmJNHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "\n",
        "\"\"\"Beautiful Soup is a Python library designed for parsing HTML and XML documents. It creates parse trees that make it straightforward to extract data from web pages,\n",
        " allowing users to navigate and search through the document structure easily.\n",
        "        Beautiful Soup is used primarily for web scraping, which involves extracting information from websites. It simplifies the process of retrieving data by providing\n",
        "intuitive methods for navigating the parse tree created from the HTML or XML content. This makes it easier to find specific elements, extract text, and manipulate the document\n",
        "as needed. Additionally, Beautiful Soup can handle poorly structured HTML (often referred to as “tag soup”), making it a versatile tool for developersworking with various web\n",
        "content formats.\"\"\""
      ],
      "metadata": {
        "id": "APvxV3izJNKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "\n",
        "\"\"\"Flask is used in web scraping projects because it provides a lightweight and flexible framework that allows developers to quickly build web applications and APIs.\n",
        "Its simplicity enables rapid development, while its extensibility supports various functionalities needed for scraping tasks, such as database integration and form handling.\n",
        "Additionally, Flask’s strong community support offers ample resources and plugins that streamline the development process, making it an ideal choice for creating efficient\n",
        "and scalable web scraping solutions.\"\"\""
      ],
      "metadata": {
        "id": "HDDaoTrUKvz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5\n",
        "\n",
        "\"\"\"\n",
        "1) Amazon EC2 (Elastic Compute Cloud)\n",
        "Amazon EC2 provides resizable compute capacity in the cloud. It allows users to launch virtual servers, known as instances, on-demand. This service is essential for\n",
        "running applications and services without the need for physical hardware, enabling scalability and flexibility.\n",
        "\n",
        "2) Amazon RDS (Relational Database Service)\n",
        "Amazon RDS simplifies the setup, operation, and scaling of relational databases in the cloud. It automates tasks such as backups, patching, and replication, allowing developers\n",
        "to focus on their applications rather than database management. RDS supports multiple database engines like MySQL, PostgreSQL, and SQL Server.\n",
        "\n",
        "3) Amazon S3 (Simple Storage Service)\n",
        "Amazon S3 is an object storage service that offers high durability and availability for storing data in the cloud. It is used to store application assets such as images, videos,\n",
        "backups, and logs securely while providing easy access through a web interface or API.\n",
        "\n",
        "4) AWS Lambda\n",
        "AWS Lambda is a serverless computing service that runs code in response to events without provisioning or managing servers. It automatically scales based on demand and charges\n",
        "only for the compute time consumed. This service is ideal for executing backend processes triggered by user actions or other AWS services.\n",
        "\n",
        "5) Amazon CloudFront\n",
        "Amazon CloudFront is a content delivery network (CDN) that speeds up the distribution of static and dynamic web content to users globally by caching copies at edge locations\n",
        "around the world. It enhances performance by reducing latency and improving load times for end-users.\n",
        "\n",
        "6) AWS IAM (Identity and Access Management)\n",
        "AWS IAM allows you to manage access to AWS services securely. It enables you to create users, groups, roles, and permissions to control who can access specific resources\n",
        "within your AWS environment while adhering to security best practices.\n",
        "\n",
        "7) Amazon VPC (Virtual Private Cloud)\n",
        "Amazon VPC allows you to create a logically isolated section of the AWS cloud where you can define your own network configuration including IP address ranges, subnets, route tables,\n",
        "and network gateways. This service enhances security by controlling inbound and outbound traffic.\n",
        "\n",
        "8) Amazon SNS (Simple Notification Service)\n",
        "Amazon SNS is a fully managed messaging service that enables message delivery between distributed systems or microservices using a publish/subscribe model. It can send notifications\n",
        "via SMS, email, or trigger other AWS services like Lambda when certain events occur.\n",
        "\n",
        "9) AWS Step Functions\n",
        "AWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services into serverless workflows so you can build applications quickly using\n",
        "visual workflows that define each step of your process.\n",
        "\n",
        "10) Amazon DynamoDB\n",
        "Amazon DynamoDB is a fully managed NoSQL database service that provides fast performance at any scale with built-in security features like encryption at rest and automatic backups.\n",
        "It’s suitable for applications requiring low-latency data access with flexible schema design.\"\"\""
      ],
      "metadata": {
        "id": "pROh_SYgLKld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}